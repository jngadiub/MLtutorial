{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Jet Tagging with **Interaction Networks** \n",
    "\n",
    "---\n",
    "In this notebook, we perform a Jet identification task using a graph-based multiclass classifier with INs.\n",
    "\n",
    "The problem consists in identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
    "based on a jet image, i.e., a 2D histogram of the transverse momentum ($p_T$) deposited in each of 100x100\n",
    "bins of a square window of the ($\\eta$, $\\phi$) plane, centered along the jet axis.\n",
    "\n",
    "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
    "\n",
    "For details on the dataset, see Notebook1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage \n",
    "\n",
    "To type shell commands, we start the command line with !\n",
    "\n",
    "**nb, if you are running locally and you have already downloaded the datasets you can skip the cell below and, if needed, change the paths later to point to the folder with your previous download of the datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://cernbox.cern.ch/index.php/s/xmTytsMPvCEA6Ar/download -o Data-MLtutorial.tar.gz\n",
    "! tar -xvzf Data-MLtutorial.tar.gz \n",
    "! ls Data-MLtutorial/JetDataset/\n",
    "! rm Data-MLtutorial.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([])\n",
    "jetList = np.array([])\n",
    "# we cannot load all data on Colab. So we just take a few files\n",
    "datafiles = ['Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "# if you are running locallt, you can use the full dataset doing\n",
    "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    # for pT, eta, phi\n",
    "    #myJetList = np.array(f.get(\"jetConstituentList\")[:,:,[5,8,11]])\n",
    "    myJetList = np.array(f.get(\"jetConstituentList\"))\n",
    "    # for px, py, pz\n",
    "    #myJetList = np.array(f.get(\"jetConstituentList\")[:,:,[0,1,2]])\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetList = np.concatenate([jetList, myJetList], axis=0) if jetList.size else myJetList\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "    del myJetList, mytarget\n",
    "    f.close()\n",
    "print(target.shape, jetList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch Cross Entropy doesn't support one-hot encoding\n",
    "target = np.argmax(target, axis=1)\n",
    "# the dataset is N_jets x N_particles x N_features\n",
    "# the IN wants N_jets x N_features x N_particles\n",
    "jetList = np.swapaxes(jetList, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 50K with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images.\n",
    "\n",
    "---\n",
    "\n",
    "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nParticle = 30\n",
    "jetList = jetList[:,:,:nParticle]\n",
    "print(jetList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(jetList, target, test_size=0.33)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "del jetList, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a GPU is available. Otherwise run on CPU\n",
    "device = 'cpu'\n",
    "args_cuda = torch.cuda.is_available()\n",
    "if args_cuda: device = \"cuda\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to pytorch\n",
    "X_train = Variable(torch.FloatTensor(X_train)).to(device)\n",
    "X_val = Variable(torch.FloatTensor(X_val)).to(device)\n",
    "y_train = Variable(torch.LongTensor(y_train).long()).to(device)\n",
    "y_val = Variable(torch.LongTensor(y_val).long()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the IN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.P = 16 # number of features\n",
    "        self.N = nParticle # number of particles\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.De = 8 # dimensionality of De learned representation\n",
    "        self.Do = 8 # number of engineered features\n",
    "        self.n_targets = 5 # number of target classes\n",
    "        self.assign_matrices() # build Rr and Rs\n",
    "\n",
    "        # build netwok\n",
    "        self.batchnorm_x = nn.BatchNorm1d(self.P)\n",
    "        self.hidden = 10\n",
    "        self.fr1 = nn.Linear(2 * self.P, self.hidden).to(device)\n",
    "        self.fr2 = nn.Linear(self.hidden, self.hidden).to(device)\n",
    "        self.fr3 = nn.Linear(self.hidden, self.De).to(device)\n",
    "        self.fo1 = nn.Linear(self.P + self.De, self.hidden).to(device)\n",
    "        self.fo2 = nn.Linear(self.hidden, self.hidden).to(device)\n",
    "        self.fo3 = nn.Linear(self.hidden, self.Do).to(device)\n",
    "        self.fc1 = nn.Linear(self.Do, self.hidden).to(device)\n",
    "        self.fc2 = nn.Linear(self.hidden, self.hidden).to(device)\n",
    "        self.fc3 = nn.Linear(self.hidden, self.n_targets).to(device)\n",
    "             \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = Variable(self.Rr).to(device)\n",
    "        self.Rs = Variable(self.Rs).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm_x(x) # [batch, P, N]\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        C = torch.cat([x, Ebar], 1)\n",
    "        del Ebar\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.De)))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "        # sum over constituents\n",
    "        O = torch.sum(O,1)\n",
    "        ### Classification MLP ###\n",
    "        N = nn.functional.relu(self.fc1(O.view(-1, self.Do)))\n",
    "        N = nn.functional.relu(self.fc2(N))\n",
    "        del O\n",
    "        N = self.fc3(N)\n",
    "        return N\n",
    "\n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        prod = torch.mm(x.reshape(x_shape[0]*x_shape[1], x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "        return prod\n",
    "\n",
    "def get_sample(training, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = np.random.choice(ind, 50000)\n",
    "    return training[chosen_ind], target[chosen_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 800\n",
    "batch_size = 100\n",
    "patience =  10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphNet()\n",
    "gnn.to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)\n",
    "\n",
    "loss_train = np.zeros(n_epochs)\n",
    "acc_train = np.zeros(n_epochs)\n",
    "loss_val = np.zeros(n_epochs)\n",
    "acc_val = np.zeros(n_epochs)\n",
    "for i in range(n_epochs):\n",
    "    print(\"Epoch %s\" % i)\n",
    "    for j in range(0, X_train.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        out = gnn(X_train[j:j + batch_size,:,:])\n",
    "        target = y_train[j:j + batch_size]\n",
    "        l = loss(out, target)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_train[i] += l.cpu().data.numpy()*batch_size\n",
    "    loss_train[i] = loss_train[i]/X_train.shape[0]\n",
    "    #acc_train[i] = stats(predicted, Y_val)\n",
    "    #### val loss & accuracy\n",
    "    for j in range(0, X_val.size()[0], batch_size):\n",
    "        out_val = gnn(X_val[j:j + batch_size])\n",
    "        target_val =  y_val[j:j + batch_size]\n",
    "        \n",
    "        l_val = loss(out_val,target_val)\n",
    "        loss_val[i] += l_val.cpu().data.numpy()*batch_size\n",
    "    loss_val[i] = loss_val[i]/X_val.shape[0]\n",
    "    print(\"Training   Loss: %f\" %l.cpu().data.numpy())\n",
    "    print(\"Validation Loss: %f\" %l_val.cpu().data.numpy())\n",
    "    if all(loss_val[max(0, i - patience):i] > min(np.append(loss_val[0:max(0, i - patience)], 200))) and i > patience:\n",
    "        print(\"Early Stopping\")\n",
    "        break\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = list(range((loss_train > 0.).sum()))\n",
    "plt.figure()\n",
    "plt.plot(epoch_number, loss_train[loss_train>0.],label='Training Loss')\n",
    "plt.plot(epoch_number, loss_val[loss_train>0.],label='Validation Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "n_batches_val = int(X_val.size()[0]/batch_size)\n",
    "if args_cuda:    \n",
    "    for j in torch.split(X_val, n_batches_val):\n",
    "        a = gnn(j).cpu().data.numpy()\n",
    "        lst.append(a)\n",
    "else:\n",
    "    for j in torch.split(X_val, n_batches_val):\n",
    "        a = gnn(j).cpu().data.numpy()\n",
    "        lst.append(a)\n",
    "predicted = Variable(torch.FloatTensor(np.concatenate(lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no softmax in the output layer. We have to put it by \n",
    "predicted = torch.nn.functional.softmax(predicted, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val = predicted.data.numpy()\n",
    "true_val = y_val.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "#### get the ROC curves\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "plt.figure()\n",
    "for i, label in enumerate(labels):\n",
    "        fpr[label], tpr[label], threshold = roc_curve((true_val== i), predict_val[:,i])\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
