{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Jet Tagging with **Recurrent Neural Network** \n",
    "\n",
    "---\n",
    "In this notebook, we perform a Jet identification task using a multiclass classifier with a GRU unit.\n",
    "Gated Recurrent Units are one kind of RNNs. \n",
    "\n",
    "The problem consists in identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
    "based on a jet image, i.e., a 2D histogram of the transverse momentum ($p_T$) deposited in each of 100x100\n",
    "bins of a square window of the ($\\eta$, $\\phi$) plane, centered along the jet axis.\n",
    "\n",
    "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
    "\n",
    "For details on the dataset, see Notebook1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage \n",
    "\n",
    "To type shell commands, we start the command line with !\n",
    "\n",
    "nb, if you are running locally you can skip the step below and change the paths later to point to the folder with your previous download of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://cernbox.cern.ch/index.php/s/xmTytsMPvCEA6Ar/download -o Data-MLtutorial.tar.gz\n",
    "! tar -xvzf Data-MLtutorial.tar.gz \n",
    "! ls Data-MLtutorial/JetDataset/\n",
    "! rm Data-MLtutorial.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5\n",
      "(50000, 5) (50000, 100, 16)\n"
     ]
    }
   ],
   "source": [
    "target = np.array([])\n",
    "jetList = np.array([])\n",
    "# we cannot load all data on Colab. So we just take a few files\n",
    "datafiles = ['Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "# if you are running locallt, you can use the full dataset doing\n",
    "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    myJetList = np.array(f.get(\"jetConstituentList\"))\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetList = np.concatenate([jetList, myJetList], axis=0) if jetList.size else myJetList\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "    del myJetList, mytarget\n",
    "    f.close()\n",
    "print(target.shape, jetList.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 50000 with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images.\n",
    "\n",
    "---\n",
    "\n",
    "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33500, 100, 16) (16500, 100, 16) (33500, 5) (16500, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(jetList, target, test_size=0.33)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "del jetList, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN model\n",
    "\n",
    "A recurrent neural network (RNN) is a type of NN which processes sequential data or time series data. They are commonly used for ordinal or temporal problems, such as natural language processing (NLP). They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output.\n",
    "\n",
    "<img src=\"figures/rnn1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "In this notebook we treat the particles clustered by the jet algorithm as an ordered sequence processed through a type of RNN called [Gated Recurrent Units](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be). GRUs are improved version of standard RNN that solves the solves the vanishing gradient problem. The update and reset gates decide what information should be passed to the output making the model able to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction. The main ingredients are:\n",
    "\n",
    "- number of hidden units: the size of the hidden state *ht*\n",
    "- gates activation function (typically a sigmoid between 0 and 1 to either let no flow or complete flow of information throughout the gates)\n",
    "- current state activation function (typically a tanh between -1 and 1 to allow for increases and decreases in the state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, GRU, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureArrayLength = (X_train.shape[1],X_train.shape[2])\n",
    "dropoutRate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 10:31:08.686347: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "inputList = Input(shape=(featureArrayLength))\n",
    "x = GRU(units=40, activation=\"tanh\", recurrent_activation='sigmoid')(inputList)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "#\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "#\n",
    "x = Dense(10, activation='relu')(x)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "x = Dense(5, activation='relu')(x)\n",
    "#\n",
    "output = Dense(5, activation='softmax')(x)\n",
    "####\n",
    "model = Model(inputs=inputList, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 16)]         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 40)                6960      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                820       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,075\n",
      "Trainable params: 8,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "predict_val = model.predict(X_val)\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(labels):\n",
    "        df[label] = y_val[:,i]\n",
    "        df[label + '_pred'] = predict_val[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
